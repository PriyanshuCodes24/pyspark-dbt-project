
# ğŸš– Pyspark-DBT-Project 
### Databricks â€¢ PySpark â€¢ Delta Lake â€¢ dbt Cloud

<p align="center">
  <img src="https://img.shields.io/badge/Databricks-FF3621?style=for-the-badge&logo=databricks&logoColor=white"/>
  <img src="https://img.shields.io/badge/Apache%20Spark-E25A1C?style=for-the-badge&logo=apachespark&logoColor=white"/>
  <img src="https://img.shields.io/badge/Delta%20Lake-003B57?style=for-the-badge&logo=delta&logoColor=white"/>
  <img src="https://img.shields.io/badge/dbt-FF694B?style=for-the-badge&logo=dbt&logoColor=white"/>
</p>

---

## ğŸ“Œ Project Overview

This project demonstrates an **end-to-end modern data engineering pipeline** using **Uber trip data**, built on top of **Databricks**, **PySpark**, **Delta Lake**, and **dbt Cloud**.

The solution follows the **Medallion Architecture (Bronze â†’ Silver â†’ Gold)** and focuses on building **scalable, incremental, and production-ready data pipelines**.

### Key Capabilities
- Incremental data ingestion  
- CDC-based upserts using Delta Lake  
- PySpark-based transformations  
- dbt-powered analytics layer  
- SCD Type-2 history tracking using dbt snapshots  
- Reusable dbt macros & Jinja templates  

---

## ğŸ—ï¸ Project Architecture
```
Source Data (Uber CSV / Raw Files)
        â†“
Bronze Layer (Delta Tables)
  - Incremental ingestion
  - Raw schema preservation
        â†“
Silver Layer (Delta Tables)
  - Deduplication
  - Data cleansing
  - CDC-based UPSERTs
        â†“
Gold Layer (dbt Models)
  - Business transformations
  - Aggregations
  - SCD Type-2 dimensions
```

---

## ğŸ”§ Tech Stack

| Layer | Technology |
|------|------------|
| Storage | Delta Lake |
| Compute | Databricks |
| Processing | PySpark |
| Transformation | dbt Cloud |
| Modeling | dbt Models & Snapshots |
| Version Control | GitHub |

---

## ğŸ“‚ Data Layers

### ğŸ¥‰ Bronze Layer â€“ Raw Ingestion
- Source: Uber datasets (customers, drivers, trips, payments, vehicles, locations)
- Built using **PySpark**
- Incremental ingestion via file-based triggers
- Stored as **Delta tables**
- No transformations applied

---

### ğŸ¥ˆ Silver Layer â€“ Cleaned & Enriched
- Built using **PySpark**
- Includes:
  - Deduplication using window functions
  - Column standardization & cleansing
  - CDC-based `MERGE INTO` upserts
  - Audit columns (`process_timestamp`)
- Stored as **Delta tables**

---

### ğŸ¥‡ Gold Layer â€“ Analytics Ready
- Built using **dbt Cloud**
- Connected directly to Databricks
- Features:
  - Incremental dbt models
  - dbt macros for reusability
  - Jinja templating for dynamic SQL
  - dbt snapshots for SCD Type-2
- Optimized for BI & analytics consumption

---

## ğŸ”„ Incremental & CDC Strategy

### PySpark (Bronze â†’ Silver)
- Incremental loads using file arrival
- Change detection via `last_updated_timestamp`
- Delta Lake `MERGE` for UPSERT logic

### dbt (Silver â†’ Gold)
- Incremental models using `is_incremental()`
- Snapshot-based historical tracking
- Current vs historical records maintained

---

## ğŸ§¬ SCD Type-2 Implementation

- Implemented using **dbt snapshots**
- Automatically managed columns:
  - `dbt_valid_from`
  - `dbt_valid_to`
- Enables full historical analysis
- Business views created on top of snapshots



## ğŸ“ Repository Structure

```
pyspark-dbt-project/
â”‚
â”œâ”€â”€ analyses/
â”‚   â”œâ”€â”€ Explore.sql
â”‚   â”‚   --> Used to explore source and transformed tables,
â”‚   â”‚       validate data, and analyze lineage in dbt
â”‚
â”œâ”€â”€ macros/
â”‚   â”œâ”€â”€ generate_schema_name.sql
â”‚   â”‚   --> Custom macro to override the default dbt schema
â”‚   â”‚       naming convention in Databricks
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ silver/
â”‚   â”‚   --> dbt models built on top of Silver Delta tables
â”‚   â”‚       used for transformations and business logic
â”‚
â”œâ”€â”€ seeds/
â”‚   
â”œâ”€â”€ snapshots/
â”‚   â”œâ”€â”€ Slowly_Changin_Dimension_Type2.yml
â”‚   â”‚   --> dbt snapshot to track SCD Type-2 changes
â”‚   â”‚       for dimension tables
â”‚   â”‚
â”‚   â””â”€â”€ FactTrips.yml
â”‚       --> Snapshot to track historical changes
â”‚           in fact-level trip data
â”‚
â”œâ”€â”€ tests/
â”‚   â”‚   --> dbt tests for data quality and validation
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ custom_utils.py
â”‚   â”‚   --> Reusable PySpark transformation utilities
â”‚   â”‚       (deduplication, timestamp handling, upserts)
â”‚
â”œâ”€â”€ .gitignore
â”‚   
â”‚
â”œâ”€â”€ README.md
â”‚  
â”‚
â”œâ”€â”€ bronze_ingestion.ipynb
â”‚   --> PySpark notebook to ingest raw Uber data
â”‚       incrementally into Bronze Delta tables
â”‚
â”œâ”€â”€ dbt_project.yml
â”‚   --> Core dbt project configuration file
â”‚
â””â”€â”€ silver_transformations.ipynb
    --> PySpark notebook to clean, deduplicate,
        and UPSERT data from Bronze to Silver layer


```

---

## ğŸ¯ Key Highlights

- End-to-end lakehouse architecture
- Production-grade incremental pipelines
- Clean separation of concerns
- Real-world CDC & SCD Type-2 handling
- Strong Databricks + dbt Cloud integration

---

## ğŸš€ Future Enhancements

- dbt tests & data quality checks
- Observability & pipeline monitoring
- CI/CD for dbt deployments
- BI integration (Power BI / Tableau / Looker)

---

## ğŸ‘¤ Author

**Priyanshu Kumar Upadhyay**
GitHub: [@PriyanshuCodes24](https://github.com/PriyanshuCodes24)


---

â­ If you find this project useful, please star the repository!
